{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 888
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7696,
     "status": "ok",
     "timestamp": 1603119507471,
     "user": {
      "displayName": "Matthew Setzler",
      "photoUrl": "",
      "userId": "02266104666152286250"
     },
     "user_tz": 240
    },
    "id": "axAPIyfgliI5",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ae2a1d6e-7238-4ecf-f227-780d31db3da8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  !pip3 install note_seq pretty_midi\n",
    "  %run drive/My\\ Drive/agent-based-tonal-model/tonal-agent-model.ipynb\n",
    "else:\n",
    "  %run tonal-agent-model.ipynb\n",
    "  %run utility-functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 30295,
     "status": "ok",
     "timestamp": 1603119452765,
     "user": {
      "displayName": "Matthew Setzler",
      "photoUrl": "",
      "userId": "02266104666152286250"
     },
     "user_tz": 240
    },
    "id": "kZECnkkixCx3",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f4306a31-0c6c-4f0b-d548-6956409e758e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1603119513533,
     "user": {
      "displayName": "Matthew Setzler",
      "photoUrl": "",
      "userId": "02266104666152286250"
     },
     "user_tz": 240
    },
    "id": "F17SiU47liJP",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# UTILITY FUNCTIONS\n",
    "####################################\n",
    "\n",
    "def write_cons(df, outpath):\n",
    "    cons_df = pd.DataFrame()\n",
    "    for w in [5,10,20]:\n",
    "        cons_w = lagged_consonance(df,0,w)\n",
    "        cons_df = pd.concat((cons_df,cons_w))\n",
    "    cons_df.to_csv(outpath,index=False)\n",
    "    return\n",
    "\n",
    "'''\n",
    "WINDOW_SIZE = 5\n",
    "def gapped_consonance(notes, gap):\n",
    "    extended_window_size = gap+WINDOW_SIZE*2\n",
    "    cons_gap = [None]*len(notes)\n",
    "    for i in range(extended_window_size,len(notes)):\n",
    "        set1 = notes.iloc[(i-extended_window_size):(i-WINDOW_SIZE-gap)]\n",
    "        set2 = notes.iloc[(i-WINDOW_SIZE):i]\n",
    "        notes_comb = pd.concat((set1.notesA,set1.notesB,set2.notesA,set2.notesB)).values\n",
    "        cc = individual_consonance_set(notes_comb)\n",
    "        cons_gap[i] = cc\n",
    "    return cons_gap\n",
    "'''\n",
    "\n",
    "def gapped_consonance(notes, gap, window):\n",
    "    extended_window_size = gap+window*2\n",
    "    cons_gap = [None]*len(notes)\n",
    "    for i in range(extended_window_size,len(notes)):\n",
    "        set1 = notes.iloc[(i-extended_window_size):(i-window-gap)]\n",
    "        set2 = notes.iloc[(i-window):i]\n",
    "        pc_hist1 = note_list_to_pc_hist(pd.concat((set1.notesA,set1.notesB)).values)\n",
    "        pc_hist2 = note_list_to_pc_hist(pd.concat((set2.notesA,set2.notesB)).values)\n",
    "        cons_gap[i] = get_cons_btw(pc_hist1, pc_hist2)\n",
    "    return cons_gap\n",
    "\n",
    "def write_gapped_cons(notes, gaps, windows, outpath):\n",
    "    cons_gap_df = pd.DataFrame()\n",
    "    for gap in gaps:\n",
    "        for window in windows:\n",
    "            cc = gapped_consonance(notes,gap,window)\n",
    "            df = pd.DataFrame({'t': range(len(cc)),'cc': cc, 'gap': gap, 'window': window})\n",
    "            cons_gap_df = pd.concat((cons_gap_df, df))\n",
    "    cons_gap_df.to_csv(outpath, index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9S4DPAiwliJh"
   },
   "source": [
    "Simulation conditions.\n",
    "* temperature [random, deterministic]\n",
    "* memory [1 to very high]\n",
    "* seeded vs unseeded\n",
    "* coupled vs oneway\n",
    "* windowed and temporal discounting memory implementations\n",
    "\n",
    "1. Unseeded (notes + cons). Window, then temp decay\n",
    "2. gapped cons\n",
    "3. Seeded (notes + cons). Window, then temp decay\n",
    "4. gapped cons "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqEXwPlEliJj",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "temps = [0,1,10,20,100] # \"deterministic\" is also implemented\n",
    "temps = [5,10,15]\n",
    "windowed_mems = [1,5,20,50,100]\n",
    "decay_mems = [1,5,10,50]\n",
    "decay_mems = [1,2,3,4,5]\n",
    "NUM_TRIALS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0HJHdR9liJw"
   },
   "source": [
    "# Simulate agents maximizing consonance with variable memory\n",
    "(no self listening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "3mouRIb0liJy",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#OUTDIR = \"output-max-cons-window/\"\n",
    "\n",
    "def simulate_coupled(m, b, seeded, outname):\n",
    "    A = TonalAgent(memory_=m, gamma_=b)\n",
    "    B = TonalAgent(memory_=m, gamma_=b)\n",
    "    if seeded:\n",
    "        A.seed()\n",
    "        B.seed()\n",
    "    for i in range(500):\n",
    "        noteA = A.generate_next_note_max_cons_no_self()\n",
    "        noteB = B.generate_next_note_max_cons_no_self()\n",
    "        A.listen(noteB)\n",
    "        B.listen(noteA)\n",
    "    write_midi_file(A.prev_notes,OUTDIR+\"midi/\"+outname+'-playerA.mid')\n",
    "    write_midi_file(B.prev_notes,OUTDIR+\"midi/\"+outname+'-playerB.mid')\n",
    "    return pd.DataFrame({'t':range(len(A.prev_notes)),\n",
    "                         'notesA': A.prev_notes,\n",
    "                         'notesB': B.prev_notes})\n",
    "\n",
    "# overdubbed duo\n",
    "def run_overdubbed(ghost_notes,m,b,seeded,outname):\n",
    "    # ghost_path: ghost_notes = pd.read_csv(ghost_path)\n",
    "    ghost_notes = ghost_notes['notesB'].values\n",
    "    A = TonalAgent(memory_=m, gamma_=b)\n",
    "    if seeded: A.seed()\n",
    "    for i in range(500):\n",
    "        noteA = A.generate_next_note_max_cons_no_self()\n",
    "        A.listen(ghost_notes[i])\n",
    "    write_midi_file(A.prev_notes,OUTDIR+\"midi/\"+outname+'-live.mid')\n",
    "    write_midi_file(ghost_notes,OUTDIR+\"midi/\"+outname+'-ghost.mid')\n",
    "    return pd.DataFrame({'t':range(len(A.prev_notes)),\n",
    "                         'notesA': A.prev_notes,\n",
    "                         'notesB': ghost_notes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!mkdir output-1step-mem\n",
    "!mkdir output-1step-mem/notes\n",
    "!mkdir output-1step-mem/consonance\n",
    "!mkdir output-1step-mem/lagged-cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "ywTMyc_zliJ9",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'simulate_coupled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-254631d010d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m# unseeded coupled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"coupled-unseeded-m\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-b\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-trial'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulate_coupled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTDIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"notes/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moutname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m#write_gapped_cons(df, gaps, OUTDIR+\"gapped-cons/\"+outname+'.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'simulate_coupled' is not defined"
     ]
    }
   ],
   "source": [
    "OUTDIR = \"output-1step-mem/\"\n",
    "####################################\n",
    "# Unseeded Simulations\n",
    "####################################\n",
    "for b in [5,10,20]:\n",
    "    for m in [1]:\n",
    "        for i in range(20):\n",
    "            print(i)\n",
    "            # unseeded coupled\n",
    "            outname = \"coupled-unseeded-m\"+str(m)+\"-b\"+str(b)+'-trial'+str(i)\n",
    "            df = simulate_coupled(m,b,False,outname)\n",
    "            df.to_csv(OUTDIR+\"notes/\"+outname+\".csv\",index=False)\n",
    "            #write_gapped_cons(df, gaps, OUTDIR+\"gapped-cons/\"+outname+'.csv')\n",
    "            write_cons(df,OUTDIR+\"consonance/\"+outname+\".csv\")\n",
    "            \n",
    "            # unseeded oneway\n",
    "            df = run_overdubbed(df,m,b,False,outname.replace('coupled','oneway'))\n",
    "            df.to_csv(OUTDIR+\"notes/\"+outname.replace('coupled','oneway')+'.csv',index=False)\n",
    "            write_cons(df,OUTDIR+\"consonance/\"+outname.replace('coupled','oneway')+\".csv\")\n",
    "            #write_gapped_cons(df, gaps, OUTDIR+\"gapped-cons/\"+outname.replace('coupled','oneway')+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTeqp_17liKJ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "####################################\n",
    "# Seeded Simulations\n",
    "####################################\n",
    "for b in temps:\n",
    "    for m in windowed_mems:\n",
    "        for i in range(NUM_TRIALS):\n",
    "            # seeded coupled\n",
    "            print('seeded coupled')\n",
    "            outname = \"coupled-seeded-m\"+str(m)+\"-b\"+str(b)+'-trial'+str(i)\n",
    "            df = simulate_coupled(m,b,True,outname)\n",
    "            df.to_csv(OUTDIR+\"notes/\"+outname+\".csv\",index=False)\n",
    "            write_cons(df,OUTDIR+\"consonance/\"+outname+\".csv\")\n",
    "            \n",
    "            # seeded oneway\n",
    "            print('seeded oneway')\n",
    "            df = run_overdubbed(df,m,b,True,outname.replace('coupled','oneway'))\n",
    "            df.to_csv(OUTDIR+\"notes/\"+outname.replace('coupled','oneway')+'.csv',index=False)\n",
    "            write_cons(df,OUTDIR+\"consonance/\"+outname.replace('coupled','oneway')+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "0qYekM6AliKU"
   },
   "source": [
    "## Simulate agents with temporal decay\n",
    "\n",
    "Goal here is to manually tune the tau and softmax parameters. I really want to get those tonal basins!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "hxYAiosQliKW",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "eae3fdbd-ec26-4240-d1cb-88e5001b7665"
   },
   "source": [
    "!mkdir output-max-cons-time-decay-sanity-check\n",
    "!mkdir output-max-cons-time-decay-sanity-check/notes\n",
    "!mkdir output-max-cons-time-decay-sanity-check/midi\n",
    "!mkdir output-max-cons-time-decay-sanity-check/gapped-cons\n",
    "!mkdir output-max-cons-time-decay-sanity-check/consonance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1603119535795,
     "user": {
      "displayName": "Matthew Setzler",
      "photoUrl": "",
      "userId": "02266104666152286250"
     },
     "user_tz": 240
    },
    "hidden": true,
    "id": "4oRJthmsliKh",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "OUTDIR = \"output-max-cons-time-decay/\"\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  OUTDIR = \"drive/My Drive/agent-based-tonal-model/\"+OUTDIR\n",
    "\n",
    "def simulate_coupled(tau, b, seeded, outname):\n",
    "    A = TonalAgent(tau_=tau, gamma_=b)\n",
    "    B = TonalAgent(tau_=tau, gamma_=b)\n",
    "    if seeded:\n",
    "        A.seed()\n",
    "        B.seed()\n",
    "    for i in range(500):\n",
    "        noteA = A.generate_next_note_temporal_decay()\n",
    "        noteB = B.generate_next_note_temporal_decay()\n",
    "        A.listen(noteB)\n",
    "        B.listen(noteA)\n",
    "    write_midi_file(A.prev_notes,OUTDIR+\"midi/\"+outname+'-playerA.mid')\n",
    "    write_midi_file(B.prev_notes,OUTDIR+\"midi/\"+outname+'-playerB.mid')\n",
    "    return pd.DataFrame({'t':range(len(A.prev_notes)),\n",
    "                         'notesA': A.prev_notes,\n",
    "                         'notesB': B.prev_notes})\n",
    "\n",
    "# overdubbed duo\n",
    "def run_overdubbed(ghost_notes,tau,b,seeded,outname):\n",
    "    # ghost_path: ghost_notes = pd.read_csv(ghost_path)\n",
    "    ghost_notes = ghost_notes['notesB'].values\n",
    "    A = TonalAgent(tau_=tau, gamma_=b)\n",
    "    if seeded: A.seed()\n",
    "    for i in range(500):\n",
    "        noteA = A.generate_next_note_temporal_decay()\n",
    "        A.listen(ghost_notes[i])\n",
    "    write_midi_file(A.prev_notes,OUTDIR+\"midi/\"+outname+'-live.mid')\n",
    "    write_midi_file(ghost_notes,OUTDIR+\"midi/\"+outname+'-ghost.mid')\n",
    "    return pd.DataFrame({'t':range(len(A.prev_notes)),\n",
    "                         'notesA': A.prev_notes,\n",
    "                         'notesB': ghost_notes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3414852,
     "status": "ok",
     "timestamp": 1603123108692,
     "user": {
      "displayName": "Matthew Setzler",
      "photoUrl": "",
      "userId": "02266104666152286250"
     },
     "user_tz": 240
    },
    "id": "IKTGHrGqliKq",
    "outputId": "2303df6f-b7e7-4fd4-94c2-b74d5d3e6d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "gaps = [1,5,10,25,50,100]\n",
    "windows = [5]\n",
    "def run_simulation(tau, gamma, beg_trial, end_trial):\n",
    "    for i in range(beg_trial,end_trial):\n",
    "        print(i)\n",
    "        # unseeded coupled\n",
    "        outname = \"coupled-unseeded-tau\"+str(tau)+\"-b\"+str(gamma)+'-trial'+str(i)\n",
    "        df = simulate_coupled(tau,gamma,False,outname)\n",
    "        df.to_csv(OUTDIR+\"notes/\"+outname+\".csv\",index=False)\n",
    "        #write_gapped_cons(df, gaps, windows, OUTDIR+\"gapped-cons/\"+outname+'.csv')\n",
    "        write_cons(df,OUTDIR+\"consonance/\"+outname+\".csv\")\n",
    "            \n",
    "        # unseeded oneway\n",
    "        df = run_overdubbed(df,tau,gamma,False,outname.replace('coupled','oneway'))\n",
    "        df.to_csv(OUTDIR+\"notes/\"+outname.replace('coupled','oneway')+'.csv',index=False)\n",
    "        write_cons(df,OUTDIR+\"consonance/\"+outname.replace('coupled','oneway')+\".csv\")\n",
    "        #write_gapped_cons(df, gaps, windows, OUTDIR+\"gapped-cons/\"+outname.replace('coupled','oneway')+'.csv')\n",
    "    return\n",
    "        \n",
    "run_simulation(.1,1,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-48b50396f7a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output-max-cons-time-decay/notes/*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-48b50396f7a7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output-max-cons-time-decay/notes/*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'join'"
     ]
    }
   ],
   "source": [
    "np.unique([f.split('/')[-1].split('-')[0:3].join() for f in glob(\"output-max-cons-time-decay/notes/*.csv\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zneum3i9liK1",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "71698a77-d3b9-4912-ea2b-8856da16e44b"
   },
   "source": [
    "####################################\n",
    "# Unseeded Simulations\n",
    "####################################\n",
    "for b in temps:\n",
    "    for tau in decay_mems:\n",
    "        for i in range(5,20):\n",
    "            print(i)\n",
    "            # unseeded coupled\n",
    "            outname = \"coupled-unseeded-tau\"+str(tau)+\"-b\"+str(b)+'-trial'+str(i)\n",
    "            df = simulate_coupled(tau,b,False,outname)\n",
    "            df.to_csv(OUTDIR+\"notes/\"+outname+\".csv\",index=False)\n",
    "            #write_gapped_cons(df, gaps, windows, OUTDIR+\"gapped-cons/\"+outname+'.csv')\n",
    "            write_cons(df,OUTDIR+\"consonance/\"+outname+\".csv\")\n",
    "            \n",
    "            # unseeded oneway\n",
    "            df = run_overdubbed(df,tau,b,False,outname.replace('coupled','oneway'))\n",
    "            df.to_csv(OUTDIR+\"notes/\"+outname.replace('coupled','oneway')+'.csv',index=False)\n",
    "            write_cons(df,OUTDIR+\"consonance/\"+outname.replace('coupled','oneway')+\".csv\")\n",
    "            #write_gapped_cons(df, gaps, windows, OUTDIR+\"gapped-cons/\"+outname.replace('coupled','oneway')+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJb9UczGliK_"
   },
   "source": [
    "## Get Gapped Consonanace Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRoCaEzMliLB",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d551f047-ea93-4dff-bb0e-276ad30e409d"
   },
   "source": [
    "from glob import glob\n",
    "OUTDIR = \"output-max-cons-time-decay/\"\n",
    "notes_files = [f for f in glob(OUTDIR+\"notes/*.csv\") if 'tau50-b100' in f]\n",
    "len(notes_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGaRkRSDliLM",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d8466c82-e4b3-450c-d6d5-7172ef54f8e5"
   },
   "source": [
    "gaps = [1,5,10,50,100]\n",
    "windows = [5,10]\n",
    "temps = [5,10,15]\n",
    "taus = [1,2,3,4,5]\n",
    "notes_files = [f for f in glob(OUTDIR+\"notes/*.csv\") if 'tau50-b100' in f]\n",
    "for f in notes_files:\n",
    "    #tau = int(f.split(\"-\")[-3].replace('tau',''))\n",
    "    #temp = int(f.split(\"-\")[-2].replace('b',''))\n",
    "    #if (not temp in temps) or (not tau in taus): continue\n",
    "    print(f)\n",
    "    notes = pd.read_csv(f)\n",
    "    write_gapped_cons(notes, gaps, windows, OUTDIR+\"gapped-cons/\"+f.split('/')[-1])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vsvRkvWliLe"
   },
   "source": [
    "## Get lagged-consonance files for all notes files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBnynQCPliLf",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "from glob import glob\n",
    "\n",
    "files = glob('output-max-cons/notes/*.csv')\n",
    "lagged_files = glob('output-max-cons/lagged-consonance/*.csv')\n",
    "for f in files:\n",
    "    #df = pd.read_csv(f)\n",
    "    outpath = OUTDIR+'lagged-consonance/'+f.split('/')[-1]\n",
    "    if outpath in lagged_files: continue\n",
    "    print(f)\n",
    "    lagged_df = pd.DataFrame()\n",
    "    df = pd.read_csv(f)\n",
    "    for lag in [-10,-5,0,5,10]:\n",
    "        print(lag)\n",
    "        lagged_df = pd.concat((lagged_df,lagged_consonance(df,lag,5)))\n",
    "    lagged_df.to_csv(outpath,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYUaKvJ3liLp",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#############################\n",
    "# Utility functions for running\n",
    "# simulations\n",
    "#############################\n",
    "\n",
    "SEED_B = 10 # for seeded simulations\n",
    "NUM_ITERATIONS = 500\n",
    "\n",
    "# mutually coupled duo\n",
    "def run_coupled_duo(m, b, seeded, outname):\n",
    "    A = TonalAgent(memory_=m,base_=b)\n",
    "    B = TonalAgent(memory_=m,base_=b)\n",
    "    if seeded:\n",
    "        A.seed()\n",
    "        B.seed()\n",
    "    for i in range(NUM_ITERATIONS):\n",
    "        noteA = A.generate_next_note_listen_no_self() # change this for listen to self\n",
    "        noteB = B.generate_next_note_listen_no_self()\n",
    "        A.listen(noteB)\n",
    "        B.listen(noteA)\n",
    "    write_midi_file(A.prev_notes,outname+'-playerA.mid')\n",
    "    write_midi_file(B.prev_notes,outname+'-playerB.mid')\n",
    "    return pd.DataFrame({'t':range(len(A.prev_notes)),\n",
    "                         'notesA': A.prev_notes,\n",
    "                         'notesB': B.prev_notes})\n",
    "\n",
    "# solo\n",
    "def run_solo(m,b,seeded,outname):\n",
    "    A = TonalAgent(memory_=m,base_=b)\n",
    "    if seeded: A.seed()\n",
    "    for i in range(1000):\n",
    "        A.generate_next_note()\n",
    "    write_midi_file(A.prev_notes,outname+'.mid')\n",
    "    cons_df = individual_consonance(A.prev_notes,outpath=outname+'-consonance.csv')\n",
    "    return cons_df\n",
    "\n",
    "# overdubbed duo\n",
    "def run_overdubbed(ghost_path,m,b,seeded,outname):\n",
    "    ghost_notes = pd.read_csv(ghost_path)\n",
    "    ghost_notes = ghost_notes['notesB'].values\n",
    "    A = TonalAgent(memory_=m,base_=b)\n",
    "    if seeded: A.seed()\n",
    "    for i in range(NUM_ITERATIONS):\n",
    "        noteA = A.generate_next_note_listen_no_self()\n",
    "        A.listen(ghost_notes[i])a\n",
    "    write_midi_file(A.prev_notes,outname+'-live.mid')\n",
    "    write_midi_file(ghost_notes,outname+'-ghost.mid')\n",
    "    return pd.DataFrame({'t':range(len(A.prev_notes)),\n",
    "                         'notes-live': A.prev_notes,\n",
    "                         'notes-ghost': ghost_notes})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiVg1AFbliLz"
   },
   "source": [
    "## Simulate coupled duos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKzzXdoBliL0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "%run tonal-agent-model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCOfeUAXliL9",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "m = 1\n",
    "for b in [1,100]:\n",
    "    for i in range(20):\n",
    "        print(i)\n",
    "        outname = \"output/notes/coupled-seeded-m\"+str(m)+\"-b\"+str(b)+'-trial'+str(i)\n",
    "        df = run_coupled_duo(m,b,True,outname)\n",
    "        df.to_csv(outname+\".csv\",index=False)\n",
    "        outname = outname.replace(\"seeded\",\"unseeded\")\n",
    "        df = run_coupled_duo(m,b,False,outname)\n",
    "        df.to_csv(outname+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x32T1e6NliMF"
   },
   "source": [
    "## Simulate overdubbed duos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8MwqxH0liMG",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "m = 1\n",
    "for b in [1,100]:\n",
    "    for i in range(20):\n",
    "        print(i)\n",
    "        outname = \"output/notes/oneway-seeded-m\"+str(m)+\"-b\"+str(b)+'-trial'+str(i)\n",
    "        ghost_path = outname.replace(\"oneway\",\"coupled\")+'.csv'\n",
    "        df = run_overdubbed(ghost_path, m, b, True, outname)\n",
    "        df.to_csv(outname+\".csv\",index=False)\n",
    "        outname = outname.replace(\"seeded\",\"unseeded\")\n",
    "        ghost_path = outname.replace(\"oneway\",\"coupled\")+'.csv'\n",
    "        df = run_overdubbed(ghost_path, m, b, False, outname)\n",
    "        df.to_csv(outname+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rnz1H1cNliMP"
   },
   "source": [
    "## Analyze coupled vs overdubbed simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8b06fuHliMR",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "simulation_files = glob(\"output-max-cons/consonance/*.csv\")\n",
    "for f in simulation_files:\n",
    "    if not 'oneway' in f: continue\n",
    "    print(f)\n",
    "    notes_df = pd.read_csv(f)\n",
    "    df = combined_consonance(notes_df.iloc[:,1],\n",
    "                        notes_df.iloc[:,2])\n",
    "    outname = f.split('/')[-1]\n",
    "    df.to_csv(\"output-max-cons/consonance/\"+outname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8T8F5taZliMb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "simulation_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mgzlh7SliMi",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "notes_df.iloc[:500,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngo9h_qlliMq"
   },
   "source": [
    "## Debug -- does seeding work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVWPAYorliMr",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# debug -- does seeding work?\n",
    "\n",
    "seed_cons = []\n",
    "rand_cons = []\n",
    "for i in range(500):\n",
    "    seed_cons.append(individual_consonance_set(TonalAgent().seed()))\n",
    "    rand_cons.append(individual_consonance_set(np.random.choice(12,50)))\n",
    "\n",
    "plt.hist(seed_cons,alpha=.2)\n",
    "plt.hist(rand_cons,alpha=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "BaHnAW-NliM1",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(seed_cons))\n",
    "print(np.mean(rand_cons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQRM54guliM-"
   },
   "source": [
    "Yes, it appears to work. Average random consonance is -1.55 -- it appears to saturate at very low values around -1.91."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "jiTm3xRaliM_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "len(coupled_df)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMxV4JVLliNH"
   },
   "source": [
    "## Seeded solo agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "tvuHrMaqliNI",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run tonal-agent-model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8zrJ2m-DliNQ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "A = TonalAgent(memory_=1,base_=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "PSWnKcTDliNX",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_solo(m,seeded=False,outname='blah'):\n",
    "    playerA = TonalAgent(memory_=m,base_=10)\n",
    "    if seeded: A.seed()\n",
    "    for i in range(1000):\n",
    "        A.generate_next_note()\n",
    "    write_midi_file(A.prev_notes,outname+'.mid')\n",
    "    cons_df = individual_consonance(A.prev_notes,outpath=outname+'-consonance.csv')\n",
    "    return cons_df\n",
    "\n",
    "# cons_df = pd.DataFrame()\n",
    "for m in [1,5,20,100]:\n",
    "    print(m)\n",
    "    for i in range(20):\n",
    "        df = run_solo(m,seeded=True)\n",
    "        df['t'] = range(len(df))\n",
    "        df['memory'] = m\n",
    "        df['trial'] = i\n",
    "        cons_df = pd.concat((cons_df,df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "NdR3FL7gliNd",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cons_df['softmax_b'] = 10\n",
    "cons_df.to_csv('output/seeded-solo.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ooj-gcOxliNm",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cons_df.groupby(['memory']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWzYHOBHliNs"
   },
   "source": [
    "No apparent difference in means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kRwFMpLyliNt",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(cons_df.cA,'.')\n",
    "plt.show()\n",
    "plt.plot(cons_df.rolling(20).cA.mean(),'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DKr2vuvuliNz",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for m in np.unique(cons_df['memory']):\n",
    "    print(m)\n",
    "    cons_df_m = cons_df[cons_df.memory==m]\n",
    "    for i in np.unique(cons_df_m['trial']):\n",
    "        cons_trial = cons_df_m[cons_df_m.trial==i]\n",
    "        plt.plot(cons_trial['t'],cons_trial['cA'],'.',alpha=.2)\n",
    "    plt.plot(cons_df_m.rolling(50).cA.mean(),'.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ycjAppCliN5"
   },
   "source": [
    "looks like things are pretty much the same for these different memories. i expected that consonance would drop off for lag-1. hmm. what does make sense is that consonance starts high, decreases after about 100 gens, then evens out for the rest of the time. maybe I should consider random too, as a baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyNgMIkBliN6"
   },
   "source": [
    "## Compute Lagged Consonance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "nZAhCR3JliN8",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "window = 5\n",
    "files = glob('output-max-cons/consonance/*.csv')\n",
    "for f in files:\n",
    "    print(f)\n",
    "    df = pd.read_csv(f)\n",
    "    cons = pd.DataFrame()\n",
    "    for lag in [-10,-5,0,5,10]:\n",
    "        lagged_cons = lagged_consonance(df,lag,window)\n",
    "        cons = pd.concat((cons,lagged_cons))\n",
    "    cons.to_csv('output-max-cons/lagged-consonance/'+f.split('/')[-1],index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "simulate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
